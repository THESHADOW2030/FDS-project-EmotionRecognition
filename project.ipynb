{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the module to read from webcam \n",
    "import cv2 as cv\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below captures a video stream of the camera and displays it in the notebook. The video stream is captured using the OpenCV library. \n",
    "If we want to capture from a video file, we can use the following code:\n",
    "\n",
    "cv.VideoCapture('project_video.mp4')\n",
    "\n",
    "On the other hand, if we want to use the camera, we can use the following code:\n",
    "\n",
    "cv.VideoCapture(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoPath = \"METTERE QUI IL PATH DEL VIDEO\"\n",
    "\n",
    "if videoPath == \"METTERE QUI IL PATH DEL VIDEO\" or videoPath == \"\":\n",
    "    cap = cv.VideoCapture(0)\n",
    "else:\n",
    "    cap = cv.VideoCapture(videoPath)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we will choose if we want to dected the frontal faces, the profile or both. We will use the frontal face cascade classifier by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "haarDetectionMode = \"profile\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "haarDetectionMode = \"profileAndFrontal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "haarDetectionMode = \"frontal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveCroppedFaces = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function detect faces uses haar cascade to detect faces in the frame. \n",
    "There are two versions of haar cascades, one for profile faces and one for frontal faces.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectFaces(frame):\n",
    "\n",
    "    detecedFaces = []\n",
    "    gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
    "    #use haar cascade to detect faces\n",
    "    if haarDetectionMode == \"frontal\" or haarDetectionMode == \"profileAndFrontal\":\n",
    "        faceCascadeFrontal = cv.CascadeClassifier(cv.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "        facesFrontal = faceCascadeFrontal.detectMultiScale(gray, 1.1, 4)\n",
    "\n",
    "        detecedFaces.extend(facesFrontal)\n",
    "\n",
    "    elif haarDetectionMode == \"profile\" or haarDetectionMode == \"profileAndFrontal\":\n",
    "        faceCascadeProfile = cv.CascadeClassifier(cv.data.haarcascades + 'haarcascade_profileface.xml')\n",
    "        facesProfile = faceCascadeProfile.detectMultiScale(gray, 1.1, 4)\n",
    "\n",
    "        detecedFaces.extend(facesProfile)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    return detecedFaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawRectangleFace(frame, facesCoordinates):\n",
    "    #draw the rectangle around the face\n",
    "    frameWithRectangle = frame.copy()\n",
    "    for (x, y, w, h) in facesCoordinates:\n",
    "        cv.rectangle(frameWithRectangle, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "    return frameWithRectangle\n",
    "        \n",
    "        \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def showVideo():\n",
    "    ret,frame = cap.read()\n",
    "    \n",
    "    #flip the image\n",
    "    frame = cv.flip(frame,1)\n",
    "\n",
    "    #detect the face\n",
    "    facesCoordinates = detectFaces(frame)\n",
    "\n",
    "    #draw the rectangle around the face\n",
    "    frameWithRectangle = drawRectangleFace(frame, facesCoordinates)\n",
    "\n",
    "\n",
    "\n",
    "    #display the image\n",
    "    cv.imshow('frame',frameWithRectangle)\n",
    "    ret = True\n",
    "    if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "        ret = False\n",
    "        \n",
    "    return ret, frame, facesCoordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotImage(image):\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "QApplication: invalid style override 'adwaita' passed, ignoring it.\n",
      "\tAvailable styles: Windows, Fusion\n"
     ]
    }
   ],
   "source": [
    "ret = True\n",
    "while ret:\n",
    "    \n",
    "    #show the video with the face detected\n",
    "    ret, frame, facesCoordinates = showVideo()\n",
    "\n",
    "    #every two seconds, save the detected face\n",
    "    if ret and len(facesCoordinates) > 0:\n",
    "        for (x, y, w, h) in facesCoordinates:\n",
    "            crop_img = frame[y:y+h, x:x+w]\n",
    "            if saveCroppedFaces:\n",
    "                cv.imwrite(\".\\\\detectedFaces\\\\face\" + str(x) + str(y) + \".jpg\", crop_img)\n",
    "\n",
    "                \n",
    "            \n",
    "    \n",
    "#release the camera\n",
    "cap.release()\n",
    "#close all windows\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Convolutional Neural Network\n",
    "\n",
    "We will use a convolutional neural network to detect the emotions. \n",
    "We will train our model using the FER2013 dataset. Then, using the trained model, we will detect the emotions in the faces detected in the video stream captured by the camera or the video file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-17 18:20:29.579717: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-17 18:20:29.727640: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-11-17 18:20:29.746017: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-17 18:20:30.167686: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/theshadow/.local/lib/python3.8/site-packages/cv2/../../lib64::/home/theshadow/Documents/MLexercises/TES/lib/\n",
      "2022-11-17 18:20:30.167729: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/theshadow/.local/lib/python3.8/site-packages/cv2/../../lib64::/home/theshadow/Documents/MLexercises/TES/lib/\n",
      "2022-11-17 18:20:30.167733: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-17 18:20:30.578112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-17 18:20:30.582182: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-17 18:20:30.582275: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "import  tensorflow  as  tf\n",
    "\n",
    "from  tensorflow  import  keras\n",
    "\n",
    "#print the number of GPU\n",
    "\n",
    "print ( \"Num GPUs Available: \" ,  len ( tf.config.experimental.list_physical_devices ( 'GPU' )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
