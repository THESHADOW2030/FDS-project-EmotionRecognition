{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Convolutional Neural Network\n",
    "\n",
    "We will use a convolutional neural network to detect the emotions. \n",
    "We will train our model using the FER2013 dataset. Then, using the trained model, we will detect the emotions in the faces detected in the video stream captured by the camera or the video file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-15 15:07:24.235679: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-15 15:07:24.327110: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-12-15 15:07:24.349603: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-12-15 15:07:24.827798: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/theshadow/Documents/MLexercises/TES/lib/\n",
      "2022-12-15 15:07:24.827841: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/theshadow/Documents/MLexercises/TES/lib/\n",
      "2022-12-15 15:07:24.827844: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2022-12-15 15:07:25.658211: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-15 15:07:25.690044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-15 15:07:25.693634: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-15 15:07:25.693751: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-15 15:07:26.032379: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-15 15:07:26.032522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-15 15:07:26.032608: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-15 15:07:26.032693: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3540 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2022-12-15 15:07:26.034094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-15 15:07:26.034209: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-15 15:07:26.034284: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "import tensorflow  as  tf\n",
    "from  tensorflow  import  keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn\n",
    "\n",
    "import numpy as  np\n",
    "import matplotlib.pyplot  as  plt\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import cv2 as cv\n",
    "import pandas  as  pd\n",
    "import tqdm as tqdm\n",
    "\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "\n",
    "#print the number of GPU\n",
    "\n",
    "print ( \"Num GPUs Available: \" ,  len ( tf.config.experimental.list_physical_devices ( 'GPU' )))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and differentiate between training and testing data (they are spit in two folders, one for training and one for testing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetToUse = \"GoogleFer\" # \"CK+48\" or \"FER2013\" or \"GoogleFer\" or \"datasetGigante\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FER2013 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data\n",
    "if datasetToUse == \"FER2013\":\n",
    "        \n",
    "        path = \"./dataset/FER2013/\"\n",
    "        test_path = path + \"test/\"\n",
    "        train_path = path + \"train/\"\n",
    "\n",
    "\n",
    "        train_x = []\n",
    "        train_y = []\n",
    "        val_x = []\n",
    "        val_y = []\n",
    "\n",
    "        #load the train data\n",
    "\n",
    "        classes = { \"angry\" : 0,  \"disgust\" : 1,  \"fear\" : 2,  \"happy\" : 3,  \"sad\" : 4,  \"surprise\" : 5,  \"neutral\" : 6}\n",
    "        classesDiz2 = { 0 : \"angry\" ,  1 : \"disgust\" ,  2 : \"fear\" ,  3 : \"happy\" ,  4 : \"sad\" ,  5 : \"surprise\" ,  6 : \"neutral\" }\n",
    "\n",
    "        data_generator = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        batch_size = 32\n",
    "        # Preprocess all test images\n",
    "        train_generator = data_generator.flow_from_directory(\n",
    "                train_path,\n",
    "                target_size=(48, 48),\n",
    "                batch_size=64,\n",
    "                color_mode=\"grayscale\",\n",
    "                class_mode='categorical')\n",
    "\n",
    "        # Preprocess all train images\n",
    "        validation_generator = data_generator.flow_from_directory(\n",
    "                test_path,\n",
    "                target_size=(48, 48),\n",
    "                batch_size=64,\n",
    "                color_mode=\"grayscale\",\n",
    "                class_mode='categorical')\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x shape:  (32645, 48, 48)\n",
      "train_y shape:  (32645, 8)\n",
      "val_x shape:  (8166, 48, 48)\n",
      "val_y shape:  (8166, 8)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if datasetToUse == \"GoogleFer\":\n",
    "\n",
    "\n",
    "        path = \"./dataset/GoogleFer/\"\n",
    "        test_path = path + \"val/\"\n",
    "        train_path = path + \"train/\"\n",
    "\n",
    "        classes = { \"anger\" : 0,  \"contempt\" : 1,  \"disgust\" : 2,  \"fear\" : 3,  \"happiness\" : 4,  \"sadness\" : 5,  \"surprise\" : 6,  \"neutral\" : 7}\n",
    "\n",
    "        classesReversed = { 0 : \"anger\" ,  1 : \"contempt\" ,  2 : \"disgust\" ,  3 : \"fear\" ,  4 : \"happiness\" ,  5 : \"sadness\" ,  6 : \"surprise\" ,  7 : \"neutral\" }\n",
    "        num_classes = 8\n",
    "\n",
    "        batch_size = 32\n",
    "        \n",
    "\n",
    "\n",
    "        train_x = []\n",
    "        train_y = []\n",
    "        val_x = []\n",
    "        val_y = []\n",
    "\n",
    "        #load the train data\n",
    "\n",
    "        \n",
    "\n",
    "        contClassesTrain = { \"anger\" : 0,  \"contempt\" : 0,  \"disgust\" : 0,  \"fear\" : 0,  \"happiness\" : 0,  \"sadness\" : 0,  \"surprise\" : 0,  \"neutral\" : 0}\n",
    "        contClassesTest = { \"anger\" : 0,  \"contempt\" : 0,  \"disgust\" : 0,  \"fear\" : 0,  \"happiness\" : 0,  \"sadness\" : 0,  \"surprise\" : 0,  \"neutral\" : 0}\n",
    "\n",
    "\n",
    "\n",
    "        for i in classes:\n",
    "                #convert i to upper case\n",
    "                #i = i.upper()    \n",
    "                path = os.path.join(train_path, i.upper())\n",
    "                 \n",
    "                for img in os.listdir(path):\n",
    "                        img_array = cv2.imread (os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n",
    "                        #new_array = cv2.resize(img_array, (48, 48))\n",
    "                        train_x.append(img_array)\n",
    "                        train_y.append(classes[i])\n",
    "                        contClassesTrain[i] = contClassesTrain[i] + 1\n",
    "\n",
    "        #load the test data\n",
    "        for i in classes:\n",
    "                #i = i.upper()  \n",
    "                path = os.path.join(test_path, i.upper())\n",
    "               \n",
    "                 \n",
    "                for img in os.listdir(path):\n",
    "                        img_array = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n",
    "                        #new_array = cv2.resize(img_array, (48, 48))\n",
    "                        val_x.append(img_array)\n",
    "                        val_y.append(classes[i])\n",
    "                        contClassesTest[i] = contClassesTest[i] + 1\n",
    "\n",
    "        #convert the val_x and train_x to numpy array\n",
    "        train_x = np.array(train_x)\n",
    "        train_y = keras.utils.to_categorical(train_y, 8)\n",
    "        val_x = np.array(val_x)\n",
    "        val_y = keras.utils.to_categorical(val_y, 8)\n",
    "\n",
    "        print ( \"train_x shape: \" , train_x.shape)\n",
    "        print ( \"train_y shape: \" , train_y.shape)\n",
    "        print ( \"val_x shape: \" , val_x.shape)\n",
    "        print ( \"val_y shape: \" , val_y.shape)\n",
    "\n",
    "\n",
    "        data_generator = ImageDataGenerator(\n",
    "                                        rescale=1./255,\n",
    "                                        rotation_range=30,\n",
    "                                        width_shift_range=0.3,\n",
    "                                        height_shift_range=0.3,        \n",
    "                                        zoom_range=0.3\n",
    "                                        )\n",
    "        \n",
    "\n",
    "        #reshape the data\n",
    "        train_x = train_x.reshape(train_x.shape[0], 48, 48, 1)\n",
    "        val_x = val_x.reshape(val_x.shape[0], 48, 48, 1)\n",
    "\n",
    "        train_generator = data_generator.flow(train_x, train_y, batch_size=batch_size)\n",
    "        validation_generator = data_generator.flow(val_x, val_y, batch_size=batch_size)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        randomIndex = random.randint(0, len(train_x))\n",
    "        #plot the first image\n",
    "        plt.imshow(train_x[randomIndex].reshape(48, 48), cmap='gray')\n",
    "        #label it\n",
    "        plt.title(classesReversed[np.argmax(train_y[randomIndex])])\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "        #plot the contClassesTrain\n",
    "        plt.bar(range(len(contClassesTrain)), list(contClassesTrain.values()), align='center')\n",
    "        plt.xticks(range(len(contClassesTrain)), list(contClassesTrain.keys()))\n",
    "        plt.show()\n",
    "\n",
    "        #plot the contClassesTest\n",
    "        plt.bar(range(len(contClassesTest)), list(contClassesTest.values()), align='center')\n",
    "        plt.xticks(range(len(contClassesTest)), list(contClassesTest.keys()))\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CK+ dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CK+ dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#load the data\n",
    "if datasetToUse == \"CK+48\":\n",
    "    dataset_path = \"./dataset/CK+48/\"\n",
    "\n",
    "    label_images = {\n",
    "        \"anger\" : [],\n",
    "        \"contempt\" : [],\n",
    "        \"disgust\" : [],\n",
    "        \"fear\" : [],\n",
    "        \"happy\" : [],\n",
    "        \"sadness\" : [],\n",
    "        \"surprise\" : []\n",
    "    }\n",
    "\n",
    "    for folder in os.listdir(dataset_path):\n",
    "        for file in os.listdir(dataset_path + folder):\n",
    "            image = cv.imread(dataset_path + folder + \"/\" + file, 0)\n",
    "\n",
    "            label_images[folder].append(np.array(image))\n",
    "\n",
    "    #for each label, take 80% of the images for training and 20% for testing\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "    val_x = []\n",
    "    val_y = []\n",
    "\n",
    "    classes = { \"anger\" : 0,  \"contempt\" : 1,  \"disgust\" : 2,  \"fear\" : 3,  \"happy\" : 4,  \"sadness\" : 5,  \"surprise\" : 6}\n",
    "    classesDiz2 = { 0 : \"anger\" ,  1 : \"contempt\" ,  2 : \"disgust\" ,  3 : \"fear\" ,  4 : \"happy\" ,  5 : \"sadness\" ,  6 : \"surprise\" }\n",
    "\n",
    "    for label in label_images:\n",
    "        cont = 0\n",
    "        for image in label_images[label]:\n",
    "            if cont < len(label_images[label]) * 0.8:\n",
    "                train_x.append(image)\n",
    "                train_y.append(classes[label])\n",
    "            else:\n",
    "                val_x.append(image)\n",
    "                val_y.append(classes[label])\n",
    "            cont += 1\n",
    "            \n",
    "\n",
    "    train_x = np.array(train_x)\n",
    "    train_y = np.array(train_y)\n",
    "\n",
    "    val_x = np.array(val_x)\n",
    "    val_y = np.array(val_y)\n",
    "\n",
    "    #print the shape of the data\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "    data_generator = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    #reshape the data\n",
    "    train_x = train_x.reshape(train_x.shape[0], 48, 48, 1)\n",
    "    val_x = val_x.reshape(val_x.shape[0], 48, 48, 1)\n",
    "\n",
    "\n",
    "    print ( \"Train data shape: \" , train_x.shape)\n",
    "    print ( \"Train labels shape: \" , train_y.shape)\n",
    "    print ( \"Test data shape: \" , val_x.shape)\n",
    "    print ( \"Test labels shape: \" , val_y.shape)\n",
    "\n",
    "\n",
    "    #convert the labels to categorical\n",
    "    train_y = keras.utils.to_categorical(train_y, 7)\n",
    "    val_y = keras.utils.to_categorical(val_y, 7)\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "    # Preprocess all test images\n",
    "    train_generator = data_generator.flow(\n",
    "            train_x,\n",
    "            train_y,    #as the batch size, select the entire dataset\n",
    "            \n",
    "    )\n",
    "    \n",
    "\n",
    "    # Preprocess all train images\n",
    "    validation_generator = data_generator.flow(\n",
    "            val_x,\n",
    "            val_y,\n",
    "            \n",
    "    )\n",
    "\n",
    "\n",
    "    num_classes = 7\n",
    "    classes = { \"anger\" : 0,  \"contempt\" : 1,  \"disgust\" : 2,  \"fear\" : 3,  \"happy\" : 4,  \"sadness\" : 5,  \"surprise\" : 6}\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Gigante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if datasetToUse == \"datasetGigante\":\n",
    "\n",
    "    path = \"./dataset/datasetGigante/\"\n",
    "\n",
    "    #load the CVS that contains the labels\n",
    "    labels = pd.read_csv(path + \"labels.csv\")\n",
    "\n",
    "   #plot the first rows of the labels\n",
    "    print (labels.head())\n",
    "\n",
    "    \n",
    "    #get the number of unique labels\n",
    "    print ( \"Number of unique labels: \" , len(labels.label.unique()))\n",
    "\n",
    "    classes = { \"anger\" : 0,  \"contempt\" : 1,  \"disgust\" : 2,  \"fear\" : 3,  \"happy\" : 4,  \"sad\" : 5,  \"surprise\" : 6,  \"neutral\" : 7}\n",
    "\n",
    "    classesReversed = { 0 : \"anger\" ,  1 : \"contempt\" ,  2 : \"disgust\" ,  3 : \"fear\" ,  4 : \"happy\" ,  5 : \"sad\" ,  6 : \"surprise\" ,  7 : \"neutral\" }\n",
    "\n",
    "    #create a dictionary that contains as key the label and as value an empty list\n",
    "    label_images = {\n",
    "        \"anger\" : [],\n",
    "        \"contempt\" : [],\n",
    "        \"disgust\" : [],\n",
    "        \"fear\" : [],\n",
    "        \"happy\" : [],\n",
    "        \"sad\" : [],\n",
    "        \"surprise\" : [],\n",
    "        \"neutral\" : []\n",
    "    }\n",
    "\n",
    "    for folder in os.listdir(path):\n",
    "\n",
    "        #check whether the folder is actually a folder\n",
    "        if not os.path.isdir(path + folder):\n",
    "            continue\n",
    "        #use tqdm to show the progress\n",
    "        print ( \"Loading images from \" , folder)\n",
    "        for file in tqdm.tqdm(os.listdir(path + folder)):\n",
    "            #the image is in color\n",
    "            imagePath = path + folder + \"/\" + file\n",
    "            imageName = folder + \"/\" + file\n",
    "            \n",
    "            #in the csv, there are two columns: the image name called pth and the label\n",
    "            #get the label of the image\n",
    "            label = labels.loc[labels['pth'] == imageName, 'label'].iloc[0]\n",
    "            \n",
    "            #load the image is in color\n",
    "            image = cv.imread(imagePath, cv.IMREAD_GRAYSCALE)\n",
    "            #resize the image\n",
    "            image = cv.resize(image, (48, 48))\n",
    "            \n",
    "            #add the image to the dictionary\n",
    "            label_images[label].append(np.array(image))\n",
    "\n",
    "    #plot 5 images per label\n",
    "    for label in label_images:\n",
    "        for i in range(5):\n",
    "            plt.imshow(label_images[label][i], cmap = \"gray\")\n",
    "            plt.title(label)\n",
    "            plt.show()\n",
    "\n",
    "    \n",
    "\n",
    "    #for each label, take 80% of the images for training and 20% for testing\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "    val_x = []\n",
    "    val_y = []\n",
    "    for label in label_images:\n",
    "        cont = 0\n",
    "        for image in label_images[label]:\n",
    "            if cont < len(label_images[label]) * 0.8:\n",
    "                train_x.append(image)\n",
    "                train_y.append(classes[label])\n",
    "            else:\n",
    "                val_x.append(image)\n",
    "                val_y.append(classes[label])\n",
    "            cont += 1\n",
    "    train_x = np.array(train_x)\n",
    "    train_y = np.array(train_y)\n",
    "    val_x = np.array(val_x)\n",
    "    val_y = np.array(val_y)\n",
    "    #reshape the data\n",
    "    train_x = train_x.reshape(train_x.shape[0], 48, 48, 1)\n",
    "    val_x = val_x.reshape(val_x.shape[0], 48, 48, 1)\n",
    "    #print the shape of the data\n",
    "    print ( \"Train data shape: \" , train_x.shape)\n",
    "    print ( \"Train labels shape: \" , train_y.shape)\n",
    "    print ( \"Test data shape: \" , val_x.shape)\n",
    "    print ( \"Test labels shape: \" , val_y.shape)\n",
    "    #convert the labels to categorical\n",
    "    train_y = keras.utils.to_categorical(train_y, 8)\n",
    "    val_y = keras.utils.to_categorical(val_y, 8)\n",
    "\n",
    "    data_generator = ImageDataGenerator(rescale=1./255)\n",
    "    # Preprocess all test images\n",
    "    train_generator = data_generator.flow(\n",
    "            train_x,\n",
    "            train_y,    #as the batch size, select the entire dataset\n",
    "    )\n",
    "    # Preprocess all train images\n",
    "    validation_generator = data_generator.flow(\n",
    "            val_x,\n",
    "            val_y,\n",
    "    )\n",
    "    num_classes = 8\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AffectNet dataset\n",
    "https://www.kaggle.com/datasets/tom99763/affectnethq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Convolution2D, BatchNormalization, Activation, AveragePooling2D, GlobalAveragePooling2D, MaxPool2D\n",
    "from keras.models import Sequential\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape=(48, 48, 1)\n",
    "weight_decay = 1e-4\n",
    "\n",
    "modelType = \"EmotionNet\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def EmotionNet():\n",
    "\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), input_shape=input_shape))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay)))\n",
    "\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(256, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(256, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(512, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(512, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, kernel_regularizer=keras.regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(512, kernel_regularizer=keras.regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(num_classes, kernel_regularizer=keras.regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    return model\n",
    "    \n",
    "\n",
    "\n",
    "def modelFromYoutube():\n",
    "\n",
    "    model = keras.models.Sequential()\n",
    "\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def ResNet():\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    pass\n",
    "                \n",
    "# create the model\n",
    "\n",
    "if modelType == \"EmotionNet\":\n",
    "    model = EmotionNet()\n",
    "elif modelType == \"modelFromYoutube\":\n",
    "    model = modelFromYoutube()\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#train the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(lr=0.0001), metrics=['accuracy'])\n",
    "\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    min_delta=0.00005,\n",
    "    patience=6,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_accuracy',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "    early_stopping,\n",
    "    lr_scheduler,\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "train_x = train_x / 255.\n",
    "val_x = val_x / 255.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_generator, validation_data=validation_generator, epochs=50, callbacks=callbacks, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.evaluate(val_x, val_y))\n",
    "\n",
    "print(model.evaluate(validation_generator))\n",
    "\n",
    "#plot the accuracy and loss\n",
    "plt.plot(history.history['accuracy'])\n",
    "\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "\n",
    "plt.title('model accuracy')\n",
    "\n",
    "plt.ylabel('accuracy')\n",
    "\n",
    "plt.xlabel('epoch')\n",
    "\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model as model name\n",
    "model.save(\"modelEmotionNet+GoogleFer66.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup the application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the module to read from webcam \n",
    "import cv2 as cv\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "#import accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below captures a video stream of the camera and displays it in the notebook. The video stream is captured using the OpenCV library. \n",
    "If we want to capture from a video file, we can use the following code:\n",
    "\n",
    "cv.VideoCapture('project_video.mp4')\n",
    "\n",
    "On the other hand, if we want to use the camera, we can use the following code:\n",
    "\n",
    "cv.VideoCapture(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'anger', 1: 'contempt', 2: 'disgust', 3: 'fear', 4: 'happiness', 5: 'sadness', 6: 'surprise', 7: 'neutral'}\n"
     ]
    }
   ],
   "source": [
    "videoPath = \"METTERE QUI IL PATH DEL VIDEO\"\n",
    "\n",
    "if videoPath == \"METTERE QUI IL PATH DEL VIDEO\" or videoPath == \"\":\n",
    "    cap = cv.VideoCapture(0)\n",
    "else:\n",
    "    cap = cv.VideoCapture(videoPath)\n",
    "\n",
    "#there are 8 emotions\n",
    "num_classes = 8\n",
    "\n",
    "classes = classesReversed\n",
    "print(classes)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we will choose if we want to dected the frontal faces, the profile or both. We will use the frontal face cascade classifier by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haarDetectionMode = \"profile\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haarDetectionMode = \"profileAndFrontal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "haarDetectionMode = \"frontal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveCroppedFaces = False\n",
    "predictEmotions = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function detect faces uses haar cascade to detect faces in the frame. \n",
    "There are two versions of haar cascades, one for profile faces and one for frontal faces.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectFaces(frame):\n",
    "\n",
    "    detectedFaces = []\n",
    "\n",
    "\n",
    "    #use haar cascade to detect faces\n",
    "    if haarDetectionMode == \"frontal\" or haarDetectionMode == \"profileAndFrontal\":\n",
    "        faceCascadeFrontal = cv.CascadeClassifier(cv.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "        facesFrontal = faceCascadeFrontal.detectMultiScale(frame, 1.1, 4)\n",
    "\n",
    "        detectedFaces.extend(facesFrontal)\n",
    "\n",
    "    elif haarDetectionMode == \"profile\" or haarDetectionMode == \"profileAndFrontal\":\n",
    "        faceCascadeProfile = cv.CascadeClassifier(cv.data.haarcascades + 'haarcascade_profileface.xml')\n",
    "        facesProfile = faceCascadeProfile.detectMultiScale(frame, 1.1, 4)\n",
    "\n",
    "        detectedFaces.extend(facesProfile)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    return detectedFaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawRectangleFace(frame, facesCoordinates):\n",
    "    #draw the rectangle around the face\n",
    "    frameWithRectangle = frame.copy()\n",
    "    for (x, y, w, h) in facesCoordinates:\n",
    "        cv.rectangle(frameWithRectangle, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        \n",
    "    return frameWithRectangle\n",
    "        \n",
    "        \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotImage(image):\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictEmotion(face):\n",
    "    #resize the image\n",
    "    face = cv.resize(face, (48, 48))\n",
    "\n",
    "    #convert the image to float\n",
    "    face = face.astype('float32')\n",
    "    #normalize the image\n",
    "    face /= 255\n",
    "    #reshape the image\n",
    "    face = face.reshape(1, 48, 48, 1)\n",
    "    #predict the emotion\n",
    "    emotion = model.predict(face)\n",
    "    return emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-15 15:08:00.183197: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-15 15:08:00.183355: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-15 15:08:00.183435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-15 15:08:00.183584: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-15 15:08:00.183670: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-15 15:08:00.183740: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3540 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "if predictEmotions:\n",
    "    #load the model\n",
    "    model = keras.models.load_model('modelEmotionNet+GoogleFer66.h5')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeEmotionOnImage(frame, facesCoordinates, emotions):\n",
    "    frameWithEmotion = frame.copy()\n",
    "    for (x, y, w, h), emotion in zip(facesCoordinates, emotions):\n",
    "        #get the emotion with the highest probability and write the confidence\n",
    "        emotion1 = classes[np.argmax(emotion)]\n",
    "        confidence = np.max(emotion)\n",
    "        cv.putText(frameWithEmotion, emotion1 + \" \" + str(confidence), (x, y), cv.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv.LINE_AA)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    return frameWithEmotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def showVideo():\n",
    "    ret,frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(\"Unable to capture video\")\n",
    "        exit()\n",
    "        \n",
    "    \n",
    "    #flip the image\n",
    "    frame = cv.flip(frame,1)\n",
    "    grayFrame = frame\n",
    "    #convert the image to grayscale\n",
    "    if frame is not None:\n",
    "        grayFrame = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "    #detect the face\n",
    "    facesCoordinates = detectFaces(grayFrame)\n",
    "\n",
    "    #draw the rectangle around the face\n",
    "    frameToShow = drawRectangleFace(frame, facesCoordinates)\n",
    "\n",
    "    if predictEmotions:\n",
    "\n",
    "        emotions = []\n",
    "        for (x, y, w, h) in facesCoordinates:\n",
    "            #crop the face\n",
    "            face = grayFrame[y:y+h, x:x+w]\n",
    "            face = cv.resize(face, (48, 48))\n",
    "            #convert to numpy array\n",
    "            face = np.array(face)\n",
    "            face.reshape(48, 48, 1)\n",
    "\n",
    "            #save the face\n",
    "            if saveCroppedFaces:\n",
    "                cv.imwrite(\"./savedFaces/face.jpg\", face)\n",
    "            \n",
    "\n",
    "\n",
    "            #predict the emotion\n",
    "            emotion = predictEmotion(face)\n",
    "            emotions.append(emotion)\n",
    "            \n",
    "\n",
    "        frameToShow = writeEmotionOnImage(frameToShow, facesCoordinates, emotions)\n",
    "\n",
    "\n",
    "    #display the image\n",
    "    cv.imshow('frame',frameToShow)\n",
    "    ret = True\n",
    "    if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "        ret = False\n",
    "        \n",
    "    return ret, frame, facesCoordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-15 15:08:10.045862: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100\n",
      "2022-12-15 15:08:10.552054: W tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2022-12-15 15:08:10.553002: W tensorflow/stream_executor/gpu/asm_compiler.cc:230] Falling back to the CUDA driver for PTX compilation; ptxas does not support CC 8.6\n",
      "2022-12-15 15:08:10.553013: W tensorflow/stream_executor/gpu/asm_compiler.cc:233] Used ptxas at ptxas\n",
      "2022-12-15 15:08:10.553049: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] UNIMPLEMENTED: ptxas ptxas too old. Falling back to the driver to compile.\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-15 15:08:11.108935: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "QApplication: invalid style override 'adwaita' passed, ignoring it.\n",
      "\tAvailable styles: Windows, Fusion\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n"
     ]
    }
   ],
   "source": [
    "ret = True\n",
    "while ret:\n",
    "    \n",
    "    \n",
    "\n",
    "    #show the video with the face detected\n",
    "    ret, frame, facesCoordinates = showVideo()\n",
    "\n",
    "\n",
    "                \n",
    "            \n",
    "    \n",
    "#release the camera\n",
    "cap.release()\n",
    "#close all windows\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction using VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the vgg16 model\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#extract the features from the images\n",
    "def extractFeatures(images):\n",
    "    features = featuatureExtractionModel.predict(images)\n",
    "    #reshape the features\n",
    "    features = features.reshape(features.shape[0], -1)\n",
    "    return features\n",
    "\n",
    "\n",
    "\n",
    "featuatureExtractionModel = VGG16(weights='imagenet', include_top=False, input_shape=(48, 48, 3))\n",
    "\n",
    "#freeze the layers\n",
    "for layer in featuatureExtractionModel.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "\n",
    "train_x = []\n",
    "train_y = []\n",
    "\n",
    "val_x = []\n",
    "val_y = []\n",
    "\n",
    "\n",
    "for folder in os.listdir(\"./dataset/GoogleFer/train/\"):\n",
    "    for file in os.listdir(\"./dataset/GoogleFer/train/\" + folder):\n",
    "        image = cv.imread(\"./dataset/GoogleFer/train/\" + folder + \"/\" + file)\n",
    "        image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "        train_x.append(image)\n",
    "        train_y.append(folder)\n",
    "\n",
    "for folder in os.listdir(\"./dataset/GoogleFer/val/\"):\n",
    "    for file in os.listdir(\"./dataset/GoogleFer/val/\" + folder):\n",
    "        image = cv.imread(\"./dataset/GoogleFer/val/\" + folder + \"/\" + file)\n",
    "        image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "        val_x.append(image)\n",
    "        val_y.append(folder)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_x = np.array(train_x)\n",
    "train_y = np.array(train_y)\n",
    "\n",
    "val_x = np.array(val_x)\n",
    "val_y = np.array(val_y)\n",
    "\n",
    "\n",
    "\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "\n",
    "print(val_x.shape)\n",
    "print(val_y.shape)\n",
    "\n",
    "\n",
    "\n",
    "train_x = extractFeatures(train_x)\n",
    "val_x = extractFeatures(val_x)\n",
    "print(\"Finished extracting features\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train a softmax classifier\n",
    "classifier = KNeighborsClassifier(n_neighbors=3)\n",
    "classifier.fit(train_x, train_y)\n",
    "\n",
    "#evaluate the model\n",
    "print(classifier.score(val_x, val_y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction using my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-15 16:27:42.884474: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-15 16:27:42.884664: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-15 16:27:42.884752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-15 16:27:42.884885: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-15 16:27:42.884978: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-15 16:27:42.885058: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3540 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Convolution2D, BatchNormalization, Activation, AveragePooling2D, GlobalAveragePooling2D, MaxPool2D\n",
    "from keras.models import Sequential\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow  as  tf\n",
    "from  tensorflow  import  keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn\n",
    "\n",
    "import numpy as  np\n",
    "import matplotlib.pyplot  as  plt\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import cv2 as cv\n",
    "import pandas  as  pd\n",
    "import tqdm as tqdm\n",
    "\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "\n",
    "#print the number of GPU\n",
    "\n",
    "print ( \"Num GPUs Available: \" ,  len ( tf.config.experimental.list_physical_devices ( 'GPU' )))\n",
    "\n",
    "#import the module to read from webcam \n",
    "import cv2 as cv\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "#import accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import seaborn as sns\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load my model \n",
    "model = keras.models.load_model('modelEmotionNet+GoogleFer66.h5')\n",
    "\n",
    "flag = True\n",
    "\n",
    "\n",
    "#create a new model starting from model until flatten\n",
    "newModel = keras.models.Model(inputs=model.input, outputs=model.get_layer('dropout_3').output)\n",
    "# print(\"New model:\")\n",
    "# newModel.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32645, 48, 48)\n",
      "(32645,)\n",
      "(8166, 48, 48)\n",
      "(8166,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_x = []\n",
    "train_y = []\n",
    "\n",
    "val_x = []\n",
    "val_y = []\n",
    "\n",
    "\n",
    "for folder in os.listdir(\"./dataset/GoogleFer/train/\"):\n",
    "    for file in os.listdir(\"./dataset/GoogleFer/train/\" + folder):\n",
    "        image = cv.imread(\"./dataset/GoogleFer/train/\" + folder + \"/\" + file)\n",
    "        image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "\n",
    "        #convert to grayscale\n",
    "        image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "        train_x.append(image)\n",
    "        train_y.append(folder)\n",
    "\n",
    "for folder in os.listdir(\"./dataset/GoogleFer/val/\"):\n",
    "    for file in os.listdir(\"./dataset/GoogleFer/val/\" + folder):\n",
    "        image = cv.imread(\"./dataset/GoogleFer/val/\" + folder + \"/\" + file)\n",
    "        image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "\n",
    "        #convert to grayscale\n",
    "        image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "        val_x.append(image)\n",
    "        val_y.append(folder)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_x = np.array(train_x)\n",
    "train_y = np.array(train_y)\n",
    "\n",
    "val_x = np.array(val_x)\n",
    "val_y = np.array(val_y)\n",
    "\n",
    "train_x = train_x / 255.0\n",
    "val_x = val_x / 255.0\n",
    "\n",
    "\n",
    "\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "\n",
    "print(val_x.shape)\n",
    "print(val_y.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractFeatures(model, images):\n",
    "    features = model.predict(images)\n",
    "    #reshape the features\n",
    "    features = features.reshape(features.shape[0], -1)\n",
    "    return features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1021/1021 [==============================] - 6s 5ms/step\n",
      "256/256 [==============================] - 1s 5ms/step\n",
      "Finished extracting features\n"
     ]
    }
   ],
   "source": [
    "train_x = extractFeatures(newModel, train_x)\n",
    "\n",
    "val_x = extractFeatures(newModel, val_x)\n",
    "print(\"Finished extracting features\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved extracted features\n",
      "(32645, 4608)\n",
      "(32645,)\n",
      "(8166, 4608)\n",
      "(8166,)\n"
     ]
    }
   ],
   "source": [
    "#now save the extracted features\n",
    "np.save(\"./extractedFeatures/train_x.npy\", train_x)\n",
    "np.save(\"./extractedFeatures/train_y.npy\", train_y)\n",
    "np.save(\"./extractedFeatures/val_x.npy\", val_x)\n",
    "np.save(\"./extractedFeatures/val_y.npy\", val_y)\n",
    "\n",
    "print(\"Saved extracted features\")\n",
    "\n",
    "#print the shape of the extracted features\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(val_x.shape)\n",
    "print(val_y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded extracted features\n",
      "(32645, 4608)\n",
      "(32645,)\n",
      "(8166, 4608)\n",
      "(8166,)\n"
     ]
    }
   ],
   "source": [
    "#load the saved features\n",
    "train_x = np.load(\"./extractedFeatures/train_x.npy\")\n",
    "train_y = np.load(\"./extractedFeatures/train_y.npy\")\n",
    "val_x = np.load(\"./extractedFeatures/val_x.npy\")\n",
    "val_y = np.load(\"./extractedFeatures/val_y.npy\")\n",
    "\n",
    "print(\"Loaded extracted features\")\n",
    "\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(val_x.shape)\n",
    "print(val_y.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32645, 4608)\n",
      "(32645,)\n",
      "(8166, 4608)\n",
      "(8166,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#the shape is (32645, 4608). \n",
    "#The first dimension is the number of images\n",
    "#The second dimension is the number of features extracted from the model\n",
    "\n",
    "train_x = train_x[:, :]\n",
    "val_x = val_x[:, :]\n",
    "\n",
    "\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(val_x.shape)\n",
    "print(val_y.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN with k = 1\n",
      "The accuracy is: 0.6729120744550575\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       ANGER       0.57      0.50      0.53       991\n",
      "    CONTEMPT       0.95      0.95      0.95       524\n",
      "     DISGUST       0.91      0.89      0.90       571\n",
      "        FEAR       0.49      0.48      0.49      1025\n",
      "   HAPPINESS       0.85      0.82      0.84      1798\n",
      "     NEUTRAL       0.57      0.57      0.57      1240\n",
      "     SADNESS       0.49      0.57      0.53      1216\n",
      "    SURPRISE       0.76      0.77      0.76       801\n",
      "\n",
      "    accuracy                           0.67      8166\n",
      "   macro avg       0.70      0.69      0.70      8166\n",
      "weighted avg       0.68      0.67      0.67      8166\n",
      "\n",
      "KNN with k = 2\n",
      "The accuracy is: 0.6471956894440363\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       ANGER       0.46      0.60      0.52       991\n",
      "    CONTEMPT       0.90      0.96      0.93       524\n",
      "     DISGUST       0.93      0.83      0.88       571\n",
      "        FEAR       0.40      0.52      0.45      1025\n",
      "   HAPPINESS       0.82      0.85      0.84      1798\n",
      "     NEUTRAL       0.55      0.56      0.55      1240\n",
      "     SADNESS       0.57      0.36      0.44      1216\n",
      "    SURPRISE       0.88      0.63      0.74       801\n",
      "\n",
      "    accuracy                           0.65      8166\n",
      "   macro avg       0.69      0.67      0.67      8166\n",
      "weighted avg       0.66      0.65      0.65      8166\n",
      "\n",
      "KNN with k = 3\n",
      "The accuracy is: 0.6697281410727406\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       ANGER       0.48      0.56      0.52       991\n",
      "    CONTEMPT       0.93      0.93      0.93       524\n",
      "     DISGUST       0.91      0.84      0.87       571\n",
      "        FEAR       0.45      0.46      0.45      1025\n",
      "   HAPPINESS       0.86      0.85      0.86      1798\n",
      "     NEUTRAL       0.61      0.57      0.59      1240\n",
      "     SADNESS       0.52      0.53      0.52      1216\n",
      "    SURPRISE       0.82      0.74      0.78       801\n",
      "\n",
      "    accuracy                           0.67      8166\n",
      "   macro avg       0.70      0.69      0.69      8166\n",
      "weighted avg       0.68      0.67      0.67      8166\n",
      "\n",
      "KNN with k = 4\n",
      "The accuracy is: 0.6765858437423463\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       ANGER       0.54      0.54      0.54       991\n",
      "    CONTEMPT       0.91      0.95      0.93       524\n",
      "     DISGUST       0.93      0.82      0.87       571\n",
      "        FEAR       0.47      0.47      0.47      1025\n",
      "   HAPPINESS       0.87      0.86      0.86      1798\n",
      "     NEUTRAL       0.58      0.63      0.60      1240\n",
      "     SADNESS       0.51      0.53      0.52      1216\n",
      "    SURPRISE       0.83      0.71      0.76       801\n",
      "\n",
      "    accuracy                           0.68      8166\n",
      "   macro avg       0.70      0.69      0.69      8166\n",
      "weighted avg       0.68      0.68      0.68      8166\n",
      "\n",
      "KNN with k = 5\n",
      "The accuracy is: 0.6783002694097477\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       ANGER       0.55      0.53      0.54       991\n",
      "    CONTEMPT       0.94      0.92      0.93       524\n",
      "     DISGUST       0.91      0.84      0.88       571\n",
      "        FEAR       0.47      0.46      0.47      1025\n",
      "   HAPPINESS       0.86      0.86      0.86      1798\n",
      "     NEUTRAL       0.57      0.63      0.60      1240\n",
      "     SADNESS       0.51      0.55      0.53      1216\n",
      "    SURPRISE       0.82      0.73      0.77       801\n",
      "\n",
      "    accuracy                           0.68      8166\n",
      "   macro avg       0.71      0.69      0.70      8166\n",
      "weighted avg       0.68      0.68      0.68      8166\n",
      "\n",
      "KNN with k = 6\n",
      "The accuracy is: 0.6798922361009062\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       ANGER       0.56      0.53      0.54       991\n",
      "    CONTEMPT       0.90      0.92      0.91       524\n",
      "     DISGUST       0.93      0.81      0.86       571\n",
      "        FEAR       0.48      0.46      0.47      1025\n",
      "   HAPPINESS       0.87      0.87      0.87      1798\n",
      "     NEUTRAL       0.57      0.64      0.60      1240\n",
      "     SADNESS       0.52      0.56      0.54      1216\n",
      "    SURPRISE       0.83      0.72      0.77       801\n",
      "\n",
      "    accuracy                           0.68      8166\n",
      "   macro avg       0.71      0.69      0.70      8166\n",
      "weighted avg       0.69      0.68      0.68      8166\n",
      "\n",
      "KNN with k = 7\n",
      "The accuracy is: 0.6809943668870928\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       ANGER       0.57      0.51      0.54       991\n",
      "    CONTEMPT       0.92      0.91      0.91       524\n",
      "     DISGUST       0.91      0.82      0.86       571\n",
      "        FEAR       0.48      0.43      0.46      1025\n",
      "   HAPPINESS       0.88      0.86      0.87      1798\n",
      "     NEUTRAL       0.58      0.65      0.61      1240\n",
      "     SADNESS       0.50      0.59      0.54      1216\n",
      "    SURPRISE       0.82      0.73      0.78       801\n",
      "\n",
      "    accuracy                           0.68      8166\n",
      "   macro avg       0.71      0.69      0.70      8166\n",
      "weighted avg       0.69      0.68      0.68      8166\n",
      "\n",
      "KNN with k = 8\n",
      "The accuracy is: 0.6771981386235612\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       ANGER       0.58      0.51      0.54       991\n",
      "    CONTEMPT       0.91      0.92      0.91       524\n",
      "     DISGUST       0.92      0.80      0.86       571\n",
      "        FEAR       0.49      0.43      0.46      1025\n",
      "   HAPPINESS       0.87      0.86      0.87      1798\n",
      "     NEUTRAL       0.57      0.66      0.61      1240\n",
      "     SADNESS       0.49      0.58      0.53      1216\n",
      "    SURPRISE       0.82      0.72      0.77       801\n",
      "\n",
      "    accuracy                           0.68      8166\n",
      "   macro avg       0.71      0.68      0.69      8166\n",
      "weighted avg       0.69      0.68      0.68      8166\n",
      "\n",
      "KNN with k = 9\n",
      "The accuracy is: 0.6823414156257653\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       ANGER       0.59      0.51      0.55       991\n",
      "    CONTEMPT       0.92      0.91      0.91       524\n",
      "     DISGUST       0.91      0.81      0.86       571\n",
      "        FEAR       0.51      0.42      0.46      1025\n",
      "   HAPPINESS       0.88      0.86      0.87      1798\n",
      "     NEUTRAL       0.57      0.66      0.61      1240\n",
      "     SADNESS       0.49      0.60      0.54      1216\n",
      "    SURPRISE       0.81      0.74      0.77       801\n",
      "\n",
      "    accuracy                           0.68      8166\n",
      "   macro avg       0.71      0.69      0.70      8166\n",
      "weighted avg       0.69      0.68      0.68      8166\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "#train a knn classifier\n",
    "for i in range(1, 10):\n",
    "    print(\"KNN with k = \" + str(i))\n",
    "    classifier = KNeighborsClassifier(n_neighbors=i)\n",
    "    classifier.fit(train_x, train_y)\n",
    "\n",
    "    #save the model using pickle\n",
    "    filename = './models/KNN' + str(i) + '.pickle'\n",
    "    pickle.dump(classifier, open(filename, 'wb'))\n",
    "\n",
    "    pred = classifier.predict(val_x)\n",
    "    #print(pred)\n",
    "\n",
    "    #evaluate the model\n",
    "    print(\"The accuracy is: \" + str(accuracy_score(val_y, pred)))\n",
    "\n",
    "    #print the report of the model\n",
    "    print(sklearn.metrics.classification_report(val_y, pred))\n",
    "\n",
    "    #save the preds\n",
    "    np.save(\"./risults/KNN\" + str(i) + \".npy\", pred)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is: 0.6699730590252265\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       ANGER       0.58      0.52      0.55       991\n",
      "    CONTEMPT       0.88      0.88      0.88       524\n",
      "     DISGUST       0.91      0.70      0.79       571\n",
      "        FEAR       0.50      0.28      0.36      1025\n",
      "   HAPPINESS       0.91      0.84      0.87      1798\n",
      "     NEUTRAL       0.57      0.69      0.62      1240\n",
      "     SADNESS       0.47      0.65      0.55      1216\n",
      "    SURPRISE       0.73      0.81      0.77       801\n",
      "\n",
      "    accuracy                           0.67      8166\n",
      "   macro avg       0.69      0.67      0.67      8166\n",
      "weighted avg       0.68      0.67      0.67      8166\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#use a naive bayes classifier from sklearn\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(train_x, train_y)\n",
    "\n",
    "#evaluate the model\n",
    "pred = classifier.predict(val_x)\n",
    "print(\"The accuracy is: \" + str(accuracy_score(val_y, pred)))\n",
    "\n",
    "#save the model using pickle\n",
    "filename = './models/NaiveBayes.pickle'\n",
    "pickle.dump(classifier, open(filename, 'wb'))\n",
    "\n",
    "#save the preds\n",
    "np.save(\"./risults/NaiveBayes.npy\", pred)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(sklearn.metrics.classification_report(val_y, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved\n",
      "The accuracy is: 0.6705853539064414\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       ANGER       0.50      0.58      0.53       991\n",
      "    CONTEMPT       0.95      0.95      0.95       524\n",
      "     DISGUST       0.91      0.89      0.90       571\n",
      "        FEAR       0.45      0.49      0.47      1025\n",
      "   HAPPINESS       0.85      0.86      0.85      1798\n",
      "     NEUTRAL       0.59      0.54      0.57      1240\n",
      "     SADNESS       0.51      0.47      0.49      1216\n",
      "    SURPRISE       0.81      0.77      0.79       801\n",
      "\n",
      "    accuracy                           0.67      8166\n",
      "   macro avg       0.70      0.69      0.69      8166\n",
      "weighted avg       0.67      0.67      0.67      8166\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#use a svm classifier\n",
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel='linear')\n",
    "classifier.fit(train_x, train_y)\n",
    "\n",
    "#save the model using pickle\n",
    "filename = './models/SVMlinear.pickle'\n",
    "pickle.dump(classifier, open(filename, 'wb'))\n",
    "\n",
    "print(\"Model saved\")\n",
    "\n",
    "pred = classifier.predict(val_x)\n",
    "\n",
    "#evaluate the model\n",
    "print(\"The accuracy is: \" + str(accuracy_score(val_y, pred)))\n",
    "\n",
    "#save the preds\n",
    "np.save(\"./risults/SVMlinear.npy\", pred)\n",
    "\n",
    "print(sklearn.metrics.classification_report(val_y, pred))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Predicted\n",
      "The accuracy is: 0.6705853539064414\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       ANGER       0.50      0.58      0.53       991\n",
      "    CONTEMPT       0.95      0.95      0.95       524\n",
      "     DISGUST       0.91      0.89      0.90       571\n",
      "        FEAR       0.45      0.49      0.47      1025\n",
      "   HAPPINESS       0.85      0.86      0.85      1798\n",
      "     NEUTRAL       0.59      0.54      0.57      1240\n",
      "     SADNESS       0.51      0.47      0.49      1216\n",
      "    SURPRISE       0.81      0.77      0.79       801\n",
      "\n",
      "    accuracy                           0.67      8166\n",
      "   macro avg       0.70      0.69      0.69      8166\n",
      "weighted avg       0.67      0.67      0.67      8166\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load the svm linear model\n",
    "filename = './models/SVMlinear.pickle'\n",
    "classifier = pickle.load(open(filename, \"rb\"))\n",
    "\n",
    "print(\"Model loaded\")\n",
    "\n",
    "pred = classifier.predict(val_x)\n",
    "\n",
    "print(\"Predicted\")\n",
    "\n",
    "#evaluate the model\n",
    "print(\"The accuracy is: \" + str(accuracy_score(val_y, pred)))\n",
    "\n",
    "#save the preds\n",
    "np.save(\"./risults/SVMlinear.npy\", pred)\n",
    "\n",
    "print(sklearn.metrics.classification_report(val_y, pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained\n",
      "Model saved\n",
      "Degree 2   The accuracy is: 0.7116091109478325\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       ANGER       0.66      0.54      0.59       991\n",
      "    CONTEMPT       0.97      0.93      0.95       524\n",
      "     DISGUST       0.93      0.87      0.90       571\n",
      "        FEAR       0.58      0.45      0.51      1025\n",
      "   HAPPINESS       0.87      0.88      0.88      1798\n",
      "     NEUTRAL       0.62      0.65      0.64      1240\n",
      "     SADNESS       0.50      0.68      0.57      1216\n",
      "    SURPRISE       0.82      0.75      0.78       801\n",
      "\n",
      "    accuracy                           0.71      8166\n",
      "   macro avg       0.74      0.72      0.73      8166\n",
      "weighted avg       0.72      0.71      0.71      8166\n",
      "\n",
      "Model trained\n",
      "Model saved\n",
      "Degree 3   The accuracy is: 0.6987509184423218\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       ANGER       0.70      0.50      0.58       991\n",
      "    CONTEMPT       0.97      0.92      0.94       524\n",
      "     DISGUST       0.94      0.87      0.90       571\n",
      "        FEAR       0.62      0.41      0.49      1025\n",
      "   HAPPINESS       0.89      0.87      0.88      1798\n",
      "     NEUTRAL       0.64      0.62      0.63      1240\n",
      "     SADNESS       0.44      0.75      0.55      1216\n",
      "    SURPRISE       0.84      0.72      0.78       801\n",
      "\n",
      "    accuracy                           0.70      8166\n",
      "   macro avg       0.75      0.71      0.72      8166\n",
      "weighted avg       0.73      0.70      0.70      8166\n",
      "\n",
      "Model trained\n",
      "Model saved\n",
      "Degree 4   The accuracy is: 0.6675238795003674\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       ANGER       0.72      0.46      0.56       991\n",
      "    CONTEMPT       0.96      0.91      0.93       524\n",
      "     DISGUST       0.94      0.85      0.89       571\n",
      "        FEAR       0.64      0.35      0.45      1025\n",
      "   HAPPINESS       0.90      0.81      0.85      1798\n",
      "     NEUTRAL       0.67      0.54      0.60      1240\n",
      "     SADNESS       0.37      0.82      0.51      1216\n",
      "    SURPRISE       0.85      0.68      0.76       801\n",
      "\n",
      "    accuracy                           0.67      8166\n",
      "   macro avg       0.76      0.68      0.69      8166\n",
      "weighted avg       0.73      0.67      0.68      8166\n",
      "\n",
      "Model trained\n",
      "Model saved\n",
      "Degree 5   The accuracy is: 0.6238060249816312\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       ANGER       0.71      0.42      0.53       991\n",
      "    CONTEMPT       0.95      0.87      0.91       524\n",
      "     DISGUST       0.93      0.83      0.88       571\n",
      "        FEAR       0.66      0.30      0.41      1025\n",
      "   HAPPINESS       0.91      0.75      0.82      1798\n",
      "     NEUTRAL       0.68      0.43      0.53      1240\n",
      "     SADNESS       0.32      0.85      0.46      1216\n",
      "    SURPRISE       0.86      0.65      0.74       801\n",
      "\n",
      "    accuracy                           0.62      8166\n",
      "   macro avg       0.75      0.64      0.66      8166\n",
      "weighted avg       0.73      0.62      0.64      8166\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#use a svm classifier\n",
    "from sklearn.svm import SVC\n",
    "for degree in range(2, 6):\n",
    "    classifier = SVC(kernel='poly', degree=degree) #by default the degree is 3\n",
    "    classifier.fit(train_x, train_y)\n",
    "\n",
    "    print(\"Model trained\")\n",
    "\n",
    "    #save the model using pickle\n",
    "    filename = './models/SVMpoly' + str(degree) + '.pickle'\n",
    "    pickle.dump(classifier, open(filename, 'wb'))\n",
    "\n",
    "    \n",
    "\n",
    "    print(\"Model saved\")\n",
    "    pred = classifier.predict(val_x)\n",
    "\n",
    "    #evaluate the model\n",
    "    print(f\"Degree {degree}   The accuracy is: \" + str(accuracy_score(val_y, pred)))\n",
    "\n",
    "    print(sklearn.metrics.classification_report(val_y, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained\n",
      "Model saved\n",
      "The accuracy is: 0.7228753367621846\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       ANGER       0.63      0.58      0.60       991\n",
      "    CONTEMPT       0.97      0.93      0.95       524\n",
      "     DISGUST       0.91      0.88      0.90       571\n",
      "        FEAR       0.59      0.48      0.53      1025\n",
      "   HAPPINESS       0.87      0.89      0.88      1798\n",
      "     NEUTRAL       0.63      0.68      0.65      1240\n",
      "     SADNESS       0.55      0.63      0.59      1216\n",
      "    SURPRISE       0.80      0.80      0.80       801\n",
      "\n",
      "    accuracy                           0.72      8166\n",
      "   macro avg       0.74      0.73      0.74      8166\n",
      "weighted avg       0.72      0.72      0.72      8166\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#use a svm classifier\n",
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel='rbf')\n",
    "classifier.fit(train_x, train_y)\n",
    "\n",
    "print(\"Model trained\")\n",
    "\n",
    "#save the model using pickle\n",
    "filename = './models/SVMrbf.pickle'\n",
    "pickle.dump(classifier, open(filename, 'wb'))\n",
    "\n",
    "print(\"Model saved\")\n",
    "\n",
    "pred = classifier.predict(val_x)\n",
    "\n",
    "#evaluate the model\n",
    "print(\"The accuracy is: \" + str(accuracy_score(val_y, pred)))\n",
    "\n",
    "print(sklearn.metrics.classification_report(val_y, classifier.predict(val_x)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/theshadow/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved\n",
      "The accuracy is: 0.6891991182953711\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       ANGER       0.58      0.54      0.56       991\n",
      "    CONTEMPT       0.96      0.94      0.95       524\n",
      "     DISGUST       0.91      0.88      0.90       571\n",
      "        FEAR       0.50      0.51      0.50      1025\n",
      "   HAPPINESS       0.86      0.86      0.86      1798\n",
      "     NEUTRAL       0.59      0.60      0.60      1240\n",
      "     SADNESS       0.53      0.55      0.54      1216\n",
      "    SURPRISE       0.78      0.78      0.78       801\n",
      "\n",
      "    accuracy                           0.69      8166\n",
      "   macro avg       0.71      0.71      0.71      8166\n",
      "weighted avg       0.69      0.69      0.69      8166\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#use the logistic regression classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(train_x, train_y)\n",
    "\n",
    "#save the model using pickle\n",
    "filename = './models/LogisticRegression.pickle'\n",
    "pickle.dump(classifier, open(filename, 'wb'))\n",
    "\n",
    "print(\"Model saved\")\n",
    "\n",
    "pred = classifier.predict(val_x)\n",
    "\n",
    "#evaluate the model\n",
    "print(\"The accuracy is: \" + str(accuracy_score(val_y, pred)))\n",
    "\n",
    "#save the preds\n",
    "np.save(\"./risults/LogisticRegression.npy\", pred)\n",
    "\n",
    "print(sklearn.metrics.classification_report(val_y, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved\n",
      "The accuracy is: 0.7073230467793289\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       ANGER       0.61      0.57      0.59       991\n",
      "    CONTEMPT       0.94      0.90      0.92       524\n",
      "     DISGUST       0.94      0.83      0.88       571\n",
      "        FEAR       0.58      0.42      0.48      1025\n",
      "   HAPPINESS       0.86      0.89      0.88      1798\n",
      "     NEUTRAL       0.61      0.66      0.63      1240\n",
      "     SADNESS       0.52      0.64      0.57      1216\n",
      "    SURPRISE       0.81      0.80      0.81       801\n",
      "\n",
      "    accuracy                           0.71      8166\n",
      "   macro avg       0.73      0.71      0.72      8166\n",
      "weighted avg       0.71      0.71      0.71      8166\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#random forest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators=100)\n",
    "classifier.fit(train_x, train_y)\n",
    "\n",
    "#save the model using pickle\n",
    "filename = './models/RandomForest.pickle'\n",
    "pickle.dump(classifier, open(filename, 'wb'))\n",
    "\n",
    "print(\"Model saved\")\n",
    "\n",
    "pred = classifier.predict(val_x)\n",
    "\n",
    "#evaluate the model\n",
    "print(\"The accuracy is: \" + str(accuracy_score(val_y, pred)))\n",
    "\n",
    "#save the preds\n",
    "np.save(\"./risults/RandomForest.npy\", pred)\n",
    "\n",
    "print(sklearn.metrics.classification_report(val_y, pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
